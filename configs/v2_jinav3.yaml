model:
  vocab_size: 250002
  max_seq_len: 32
  hidden_dim: 768
  num_layers: 8
  num_heads: 12
  ff_dim: 3072
  dropout: 0.0
  embedding_cond_dim: 1024
  mask_token_id: 250001
  encoder_model: jinaai/jina-embeddings-v3
  decoder_tokenizer: xlm-roberta-base

training:
  batch_size: 400
  grad_accum: 4
  max_steps: 200000
  lr: 0.0001
  min_lr_ratio: 0.1
  weight_decay: 0.01
  warmup_steps: 2000
  max_grad_norm: 1.0
  log_every: 1
  eval_every: 500
  num_workers: 4
  mixed_precision: true
  ema_decay: 0.9999

data:
  data_dir: data_jinav3
  val_split: 0.01
