# v3: mmBERT-base backbone with AdaLN-Zero conditioning + jina-v3 encoder
# Full pretrained mmBERT (22 layers, 768d) with conditioning on jina-embeddings-v3
model:
  vocab_size: 256001          # mmBERT vocab (256000) + 1 mask token
  max_seq_len: 32
  hidden_dim: 768             # mmBERT hidden size
  num_layers: 22              # mmBERT num layers (loaded from pretrained)
  num_heads: 12               # mmBERT num attention heads
  ff_dim: 1152                # mmBERT intermediate size (not used, loaded from pretrained)
  dropout: 0.0
  embedding_cond_dim: 1024    # jina-embeddings-v3 output dim
  mask_token_id: 256000       # vocab_size - 1
  encoder_model: jinaai/jina-embeddings-v3
  decoder_tokenizer: jhu-clsp/mmBERT-base
  pretrained_token_embeddings: jhu-clsp/mmBERT-base
  freeze_token_embeddings: true
  tie_weights: false            # output_proj independent from frozen token_embed

training:
  batch_size: 400
  grad_accum: 4
  max_steps: 200000
  lr: 0.0001
  min_lr_ratio: 0.1
  weight_decay: 0.01
  warmup_steps: 2000
  max_grad_norm: 1.0
  log_every: 1
  eval_every: 500
  num_workers: 4
  mixed_precision: true
  ema_decay: 0.9999
  early_stop_patience: 5000

data:
  data_dir: data_mmbert_jinav3
  val_split: 0.01
